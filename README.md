# chinese_pytorch_pretrain_lm
# åŸºäºpytorchçš„ä¸­æ–‡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„DAPTå’ŒTAPT
[![License](https://img.shields.io/badge/license-Apache%202-green.svg)](https://www.apache.org/licenses/LICENSE-2.0)
[![Build Status](https://travis-ci.org/xialonghua/kotmvp.svg?branch=master)](https://travis-ci.org/xialonghua/kotmvp) 

ACL2020 Best Paperæœ‰ä¸€ç¯‡è®ºæ–‡æåå¥–ï¼Œã€ŠDonâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasksã€‹ã€‚è¿™ç¯‡è®ºæ–‡åšäº†å¾ˆå¤šè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„å®éªŒï¼Œç³»ç»Ÿçš„åˆ†æäº†è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå¯¹å­ä»»åŠ¡çš„æ•ˆæœæå‡æƒ…å†µã€‚æœ‰å‡ ä¸ªä¸»è¦ç»“è®ºï¼š
* åœ¨ç›®æ ‡é¢†åŸŸçš„æ•°æ®é›†ä¸Šç»§ç»­é¢„è®­ç»ƒï¼ˆDAPTï¼‰å¯ä»¥æå‡æ•ˆæœï¼›ç›®æ ‡é¢†åŸŸçš„è¯­æ–™ä¸RoBERTaçš„åŸå§‹é¢„è®­ç»ƒè¯­æ–™è¶Šä¸ç›¸å…³ï¼ŒDAPTæ•ˆæœåˆ™æå‡æ›´æ˜æ˜¾ã€‚

* åœ¨å…·ä½“ä»»åŠ¡çš„æ•°æ®é›†ä¸Šç»§ç»­é¢„è®­ç»ƒï¼ˆTAPTï¼‰å¯ä»¥ååˆ†â€œå»‰ä»·â€åœ°æå‡æ•ˆæœã€‚

* ç»“åˆäºŒè€…ï¼ˆå…ˆè¿›è¡ŒDAPTï¼Œå†è¿›è¡ŒTAPTï¼‰å¯ä»¥è¿›ä¸€æ­¥æå‡æ•ˆæœã€‚

* å¦‚æœèƒ½è·å–æ›´å¤šçš„ã€ä»»åŠ¡ç›¸å…³çš„æ— æ ‡æ³¨æ•°æ®ç»§ç»­é¢„è®­ç»ƒï¼ˆCurated-TAPTï¼‰ï¼Œæ•ˆæœåˆ™æœ€ä½³ã€‚

* å¦‚æœæ— æ³•è·å–æ›´å¤šçš„ã€ä»»åŠ¡ç›¸å…³çš„æ— æ ‡æ³¨æ•°æ®ï¼Œé‡‡å–ä¸€ç§ååˆ†è½»é‡åŒ–çš„ç®€å•æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œæ•ˆæœä¹Ÿä¼šæå‡ã€‚

**â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”ğŸ¤—å‰æ–¹é«˜èƒ½ğŸ¤—â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”**

å¯¹GPTã€GPT-2ã€GPTæ–‡æœ¬æ•°æ®é›†ä¸Šçš„è¯­è¨€å»ºæ¨¡åº“æ¨¡å‹è¿›è¡Œå¾®è°ƒ(æˆ–ä»å¤´å¼€å§‹è®­ç»ƒ)ã€‚ALBERT, BERT, distillbert, RoBERTa, XLNetâ€¦ GPTå’ŒGPT-2ä½¿ç”¨å› æœè¯­è¨€å»ºæ¨¡è¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒ  
(CLM)ä¸¢å¤±ï¼Œè€ŒALBERTã€BERTã€DistilBERTå’ŒRoBERTaä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡(MLM)è¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒçš„æŸå¤±ã€‚ XLNetä½¿ç”¨æ’åˆ—è¯­è¨€å»ºæ¨¡(PLM)ï¼Œå…³äºå®ƒä»¬ä¹‹é—´å·®å¼‚çš„æ›´å¤šä¿¡æ¯[æ¨¡å‹æ€»ç»“](https://huggingface.co/transformers/model_summary.html)

è¿™é‡Œæä¾›äº†ä¸¤ç»„è„šæœ¬ã€‚ ç¬¬ä¸€ä¸ªåˆ©ç”¨äº†Trainer APIã€‚ ç¬¬äºŒä¸ªå¸¦' no_trainer 'åç¼€çš„é›†åˆä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯å¹¶åˆ©ç”¨ğŸ¤—Accelerateåº“ã€‚ è¿™ä¸¤ä¸ªé›†åˆéƒ½ä½¿ç”¨ğŸ¤—Datasetsåº“ã€‚ å¦‚æœéœ€è¦å¯¹æ•°æ®é›†è¿›è¡Œé¢å¤–å¤„ç†ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è½»æ¾å®šåˆ¶å®ƒä»¬ã€‚
**PS:** æ—§ç‰ˆæœ¬çš„è„šæœ¬ `run_language_modeling.py` å¯ä»¥ä» [è¿™é‡Œ](https://github.com/huggingface/transformers/blob/main/examples/legacy/run_language_modeling.py). è·å–ã€‚

### GPT-2/GPT and causal language modeling
pre-training: causal language modeling

```bash
python run_clm.py \
    --model_name_or_path gpt2 \
    --train_file path_to_train_file \
    --validation_file path_to_validation_file \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm
```

è¿™ä½¿ç”¨å†…ç½®çš„HuggingFaceâ€œè®­ç»ƒå™¨â€è¿›è¡Œè®­ç»ƒã€‚ å¦‚æœæ‚¨æƒ³ä½¿ç”¨è‡ªå®šä¹‰çš„è®­ç»ƒå›è·¯ï¼Œæ‚¨å¯ä»¥åˆ©ç”¨æˆ–ä¿®æ”¹' run_clm_no_trainer.py 'è„šæœ¬ã€‚ æŸ¥çœ‹è„šæœ¬ä»¥è·å¾—å—æ”¯æŒçš„å‚æ•°åˆ—è¡¨ã€‚ ç¤ºä¾‹å¦‚ä¸‹: 

```bash
python run_clm_no_trainer.py \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --model_name_or_path gpt2 \
    --output_dir /tmp/test-clm
```

### RoBERTa/BERT/DistilBERT and masked language modeling

pre-training: masked language modeling.

æ ¹æ®RoBERTaçš„è®ºæ–‡ï¼Œä½¿ç”¨åŠ¨æ€å±è”½è€Œä¸æ˜¯é™æ€å±è”½ã€‚ å› æ­¤ï¼Œæ¨¡å‹å¯èƒ½æ”¶æ•›ç¨å¾®æ…¢ä¸€äº›(è¿‡åº¦æ‹Ÿåˆéœ€è¦æ›´å¤šçš„epochs)


```bash
python run_mlm.py \
    --model_name_or_path roberta-base \
    --train_file path_to_train_file \
    --validation_file path_to_validation_file \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-mlm
```

å¦‚æœæ‚¨çš„æ•°æ®é›†æ˜¯æ¯è¡Œä¸€ä¸ªæ ·æœ¬ç»„ç»‡çš„, éœ€è¦ä½¿ç”¨ `--line_by_line` 

è¿™ä½¿ç”¨å†…ç½®çš„HuggingFaceâ€œè®­ç»ƒå™¨â€è¿›è¡Œè®­ç»ƒã€‚ å¦‚æœæ‚¨æƒ³ä½¿ç”¨è‡ªå®šä¹‰çš„è®­ç»ƒå›è·¯ï¼Œæ‚¨å¯ä»¥åˆ©ç”¨æˆ–ä¿®æ”¹' run_mlm_no_trainer.py 'è„šæœ¬ã€‚ æŸ¥çœ‹è„šæœ¬ä»¥è·å¾—å—æ”¯æŒçš„å‚æ•°åˆ—è¡¨ã€‚ ç¤ºä¾‹å¦‚ä¸‹: 

```bash
python run_mlm_no_trainer.py \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --model_name_or_path roberta-base \
    --output_dir /tmp/test-mlm
```

**PS:** åœ¨TPUä¸Šï¼Œä½ åº”è¯¥ä½¿ç”¨æ ‡å¿— `--pad_to_max_length` å’Œ `--line_by_line` ç¡®ä¿æ‰€æœ‰çš„æ‰¹æ¬¡éƒ½æœ‰ç›¸åŒçš„é•¿åº¦.
 

### Whole word masking

é¦–å…ˆä½¿ç”¨LTPåˆ†è¯

```bash
export TRAIN_FILE=/path/to/train/file
export LTP_RESOURCE=/path/to/ltp/tokenizer
export BERT_RESOURCE=/path/to/bert/tokenizer
export SAVE_PATH=/path/to/data/ref.txt

python run_chinese_ref.py \
    --file_name=$TRAIN_FILE \
    --ltp=$LTP_RESOURCE \
    --bert=$BERT_RESOURCE \
    --save_path=$SAVE_PATH
```
ç„¶åæ‰§è¡Œè®­ç»ƒè„šæœ¬

```bash
export TRAIN_FILE=/path/to/train/file
export VALIDATION_FILE=/path/to/validation/file
export TRAIN_REF_FILE=/path/to/train/chinese_ref/file
export VALIDATION_REF_FILE=/path/to/validation/chinese_ref/file
export OUTPUT_DIR=/tmp/test-mlm-wwm

python run_mlm_wwm.py \
    --model_name_or_path roberta-base \
    --train_file $TRAIN_FILE \
    --validation_file $VALIDATION_FILE \
    --train_ref_file $TRAIN_REF_FILE \
    --validation_ref_file $VALIDATION_REF_FILE \
    --do_train \
    --do_eval \
    --output_dir $OUTPUT_DIR
```

### XLNet and permutation language modeling

XLNetä½¿ç”¨ä¸åŒçš„è®­ç»ƒç›®æ ‡ï¼Œå³æ’åˆ—è¯­è¨€å»ºæ¨¡ã€‚ è¿™æ˜¯ä¸€ç§è‡ªå›å½’æ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–è¾“å…¥çš„æ‰€æœ‰æ’åˆ—çš„æœŸæœ›å¯èƒ½æ€§æ¥å­¦ä¹ åŒå‘ä¸Šä¸‹æ–‡åºåˆ—åˆ†è§£é¡ºåºã€‚

ä½¿ç”¨ `--plm_probability` ç”¨äºæ’åˆ—è¯­è¨€å»ºæ¨¡çš„æ©ç tokençš„é•¿åº¦ä¸å‘¨å›´ä¸Šä¸‹æ–‡é•¿åº¦çš„æ¯”å€¼ã€‚

ä½¿ç”¨ `--max_span_length` é™åˆ¶ç”¨äºæ’åˆ—è¯­è¨€å»ºæ¨¡çš„å±è”½ä»¤ç‰Œçš„é•¿åº¦

```bash
python run_plm.py \
    --model_name_or_path=xlnet-base-cased \
    --train_file path_to_train_file \
    --validation_file path_to_validation_file \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-plm
```

å¦‚æœæ‚¨çš„æ•°æ®é›†æ˜¯æ¯è¡Œä¸€ä¸ªæ ·æœ¬ç»„ç»‡çš„, éœ€è¦ä½¿ç”¨ `--line_by_line` 

**PS:** åœ¨TPUä¸Šï¼Œä½ åº”è¯¥ä½¿ç”¨æ ‡å¿— `--pad_to_max_length` å’Œ `--line_by_line` ç¡®ä¿æ‰€æœ‰çš„æ‰¹æ¬¡éƒ½æœ‰ç›¸åŒçš„é•¿åº¦.


## Creating a model on the fly

ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹æ—¶ï¼Œé…ç½®å€¼å¯ä»¥é‡å†™  `--config_overrides`:


```bash
python run_clm.py --model_type gpt2 --tokenizer_name gpt2 \ --config_overrides="n_embd=1024,n_head=16,n_layer=48,n_positions=102" \
[...]
```

æ­¤åŠŸèƒ½ä»…åœ¨ `run_clm.py`, `run_plm.py` å’Œ `run_mlm.py`.


**æç¤º:**  run_mlm_www TrainingArguments å‚æ•°è§£æ
### å¼€å§‹
    output_dirï¼ˆï¼šobjï¼š`str`ï¼‰ï¼š
        æ¨¡å‹é¢„æµ‹å’Œæ£€æŸ¥ç‚¹çš„è¾“å‡ºç›®å½•ã€‚å¿…é¡»å£°æ˜çš„å­—æ®µã€‚
    overwrite_output_dirï¼ˆï¼šobjï¼š`bool`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸ºï¼šobjï¼š`False`ï¼‰ï¼š
        å¦‚æœä¸ºTrueï¼Œåˆ™è¦†ç›–è¾“å‡ºç›®å½•çš„å†…å®¹ã€‚ä½¿ç”¨æ­¤ç»§ç»­è®­ç»ƒï¼Œå¦‚æœ`output_dir`æŒ‡å‘æ£€æŸ¥ç‚¹ç›®å½•ã€‚
    do_trainï¼ˆï¼šobjï¼š`bool`ï¼Œ`å¯é€‰`ï¼Œé»˜è®¤ä¸ºï¼šobjï¼š`False`ï¼‰ï¼š
        æ˜¯å¦è¿›è¡Œè®­ç»ƒã€‚
    do_evalï¼ˆï¼šobjï¼š`bool`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸ºï¼šobjï¼š`False`ï¼‰ï¼š
        æ˜¯å¦åœ¨éªŒè¯é›†ä¸Šè¿è¡Œè¯„ä¼°ã€‚
    do_predictï¼ˆï¼šobjï¼š`bool`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸ºï¼šobjï¼š`False`ï¼‰ï¼š
        æ˜¯å¦åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œé¢„æµ‹ã€‚
    evaluation_strategy(`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `"no"`):
            The evaluation strategy to adopt during training. Possible values are:

                - `"no"`: No evaluation is done during training.
                - `"steps"`: Evaluation is done (and logged) every `eval_steps`.
                - `"epoch"`: Evaluation is done at the end of each epoch.
    per_device_train_batch_sizeï¼ˆï¼šobjï¼š`int`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º8ï¼‰ï¼š
        æ¯ä¸ªGPU / TPUå†…æ ¸/ CPUçš„æ‰¹å¤„ç†å¤§å°ã€‚
    per_device_eval_batch_sizeï¼ˆï¼šobjï¼š`int`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º8ï¼‰ï¼š
        æ¯ä¸ªGPU / TPUå†…æ ¸/ CPUçš„æ‰¹å¤„ç†å¤§å°ï¼Œä»¥è¿›è¡Œè¯„ä¼°ã€‚
    gradient_accumulation_stepsï¼šï¼ˆï¼šobjï¼š`int`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º1ï¼‰ï¼š
        åœ¨æ‰§è¡Œåå‘ä¼ æ’­/æ›´æ–°è¿‡ç¨‹ä¹‹å‰ï¼Œè¦ç´¯ç§¯å…¶æ¢¯åº¦çš„æ›´æ–°æ­¥éª¤æ•°ã€‚
    learning_rateï¼ˆï¼šobjï¼š`float`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º5e-5ï¼‰ï¼š
        Adamåˆå§‹å­¦ä¹ ç‡ã€‚#è¿™é‡Œä¸çŸ¥é“ä¸ºä»€ä¹ˆå¼ºè°ƒAdamï¼Ÿ
    weight_decayï¼ˆï¼šobjï¼š`float`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º0ï¼‰ï¼š
        è¦åº”ç”¨çš„æƒé‡è¡°å‡ï¼ˆå¦‚æœä¸ä¸ºé›¶ï¼‰ã€‚
    adam_epsilonï¼ˆï¼šobjï¼š`float`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º1e-8ï¼‰ï¼š
        Epsilonï¼Œç”¨äºAdamä¼˜åŒ–å™¨ã€‚
    max_grad_normï¼ˆï¼šobjï¼š`float`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º1.0ï¼‰ï¼š
        æœ€å¤§æ¸å˜èŒƒæ•°ï¼ˆç”¨äºæ¸å˜è£å‰ªï¼‰ã€‚
    num_train_epochsï¼ˆï¼šobjï¼š`float`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º3.0ï¼‰ï¼š
        è¦æ‰§è¡Œçš„è®­ç»ƒè½®æ•°æ€»æ•°ã€‚
    max_stepsï¼ˆï¼šobjï¼š`int`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º-1ï¼‰ï¼š
        å¦‚æœè®¾ç½®ä¸ºæ­£æ•°ï¼Œåˆ™è¦æ‰§è¡Œçš„è®­ç»ƒæ­¥éª¤æ€»æ•°ã€‚è¦†å†™
        ï¼šobjï¼š`num_train_epochs`ã€‚
    warmup_stepsï¼ˆï¼šobjï¼š`int`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º0ï¼‰ï¼š
        çº¿æ€§é¢„çƒ­æ‰€ç”¨çš„æ­¥æ•°ï¼ˆä»0åˆ°ï¼šlearning_rateï¼‰ã€‚
    logging_dirï¼ˆï¼šobjï¼š`str`ï¼Œ`optional`ï¼‰ï¼š
        Tensorboardæ—¥å¿—ç›®å½•ã€‚å°†é»˜è®¤ä¸º`runs / ** CURRENT_DATETIME_HOSTNAME **`ã€‚ç”¨å½“å‰æ—¶é—´æ„é€ 
    logging_first_stepï¼ˆï¼šobjï¼š`bool`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸ºï¼šobjï¼š`False`ï¼‰ï¼š
        æ˜¯å¦éœ€è¦è®°å½•å’Œè¯„ä¼°ç¬¬ä¸€ä¸ªï¼šobjï¼š`global_step`æˆ–æ²¡æœ‰ã€‚
    logging_stepsï¼ˆï¼šobjï¼š`int`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º500ï¼‰ï¼š
        ä¸¤ä¸ªæ—¥å¿—è®°å½•ä¹‹é—´çš„æ›´æ–°æ­¥éª¤æ•°ã€‚
    save_stepsï¼ˆï¼šobjï¼š`int`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º500ï¼‰ï¼š
        ä¿å­˜ä¸¤ä¸ªæ£€æŸ¥ç‚¹ä¹‹å‰çš„æ›´æ–°æ­¥éª¤æ•°ã€‚
    save_total_limitï¼ˆï¼šobjï¼š`int`ï¼Œ`Optional`ï¼‰ï¼š
        å¦‚æœè®¾ç½®å…·ä½“æ•°å€¼ï¼Œå°†é™åˆ¶æ£€æŸ¥ç‚¹çš„æ€»æ•°ã€‚åˆ é™¤ä¸­çš„æ—§æ£€æŸ¥ç‚¹
        ï¼šobjï¼š`output_dir`ã€‚
    no_cudaï¼ˆï¼šobjï¼š`bool`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸ºï¼šobjï¼š`False`ï¼‰ï¼š
        è®¾ç½®æ˜¯å¦ä¸ä½¿ç”¨CUDAï¼Œå³ä½¿æ²¡æœ‰CUDAã€‚ï¼ˆå¤§å®¶éƒ½æ˜¯æœ‰GPUçš„ï¼Œå°±ä¸è¦ç¢°è¿™ä¸ªé€‰é¡¹å•¦ï¼‰
    seedï¼ˆï¼šobjï¼š`int`ï¼Œ`å¯é€‰`ï¼Œé»˜è®¤ä¸º42ï¼‰ï¼š
        ç”¨äºåˆå§‹åŒ–çš„éšæœºç§å­ã€‚
    fp16ï¼ˆï¼šobjï¼š`bool`ï¼Œ`å¯é€‰`ï¼Œé»˜è®¤ä¸ºï¼šobjï¼š`False`ï¼‰ï¼š
        æ˜¯å¦ä½¿ç”¨16ä½æ··åˆç²¾åº¦è®­ç»ƒï¼ˆé€šè¿‡NVIDIA apexï¼‰è€Œä¸æ˜¯32ä½è®­ç»ƒã€‚
    fp16_opt_levelï¼ˆï¼šobjï¼š`str`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º'O1'ï¼‰ï¼š
        å¯¹äºfp16è®­ç»ƒï¼Œè¯·åœ¨['O0'ï¼Œ'O1'ï¼Œ'O2'å’Œ'O3']ä¸­é€‰æ‹©é¡¶ç‚¹AMPä¼˜åŒ–çº§åˆ«ã€‚æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
        åœ¨`apexæ–‡æ¡£<https://nvidia.github.io/apex/amp.html>`__ä¸­ã€‚
    local_rankï¼ˆï¼šobjï¼š`int`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º-1ï¼‰ï¼š
        åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­è¿›è¡Œè®¾ç½®ã€‚
    tpu_num_coresï¼ˆï¼šobjï¼š`int`ï¼Œ`optional`ï¼‰ï¼š
        åœ¨TPUä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œä¼šå ç”¨å¤§é‡TPUæ ¸å¿ƒï¼ˆç”±å¯åŠ¨è„šæœ¬è‡ªåŠ¨ä¼ é€’ï¼‰ã€‚
    debugï¼ˆï¼šobjï¼š`bool`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸ºï¼šobjï¼š`False`ï¼‰ï¼š
        åœ¨TPUä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ˜¯å¦æ‰“å°è°ƒè¯•æŒ‡æ ‡ã€‚
    dataloader_drop_lastï¼ˆï¼šobjï¼š`bool`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸ºï¼šobjï¼š`False`ï¼‰ï¼š
        æ˜¯å¦åˆ é™¤æœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡ã€‚
    eval_stepsï¼ˆï¼šobjï¼š`int`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º1000ï¼‰ï¼š
        ä¸¤æ¬¡è¯„ä¼°ä¹‹é—´çš„æ›´æ–°æ­¥éª¤æ•°ã€‚
    past_indexï¼ˆï¼šobjï¼š`int`ï¼Œ`optional`ï¼Œé»˜è®¤ä¸º-1ï¼‰ï¼š
        è¯¸å¦‚TransformerXL <../ model_doc / transformerxl>æˆ–docNet XLNet <../ model_doc / xlnet>ä¹‹ç±»çš„æŸäº›æ¨¡å‹å¯ä»¥
        åˆ©ç”¨è¿‡å»çš„éšè—çŠ¶æ€è¿›è¡Œé¢„æµ‹ã€‚å¦‚æœå°†æ­¤å‚æ•°è®¾ç½®ä¸ºæ­£æ•´æ•°ï¼Œåˆ™
        åœ¨å…³é”®å­—å‚æ•°``mems``ä¸‹ï¼Œ``Trainer`` å°†ä½¿ç”¨ç›¸åº”çš„è¾“å‡ºï¼ˆé€šå¸¸æ˜¯ç´¢å¼•2ï¼‰ä½œä¸ºè¿‡å»çš„çŠ¶æ€å¹¶å°†å…¶è¾“å…¥ä¸‹ä¸€ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ã€‚
    ignore_data_skip (`bool`, *optional*, defaults to `False`):
            When resuming training, whether or not to skip the epochs and batches to get the data loading at the same
            stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step
            can take a long time) but will not yield the same results as the interrupted training would have.
    --resume_from_checkpoint ./output/checkpoint-56500 
### ç»“æŸ
    