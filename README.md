# chinese_pytorch_pretrain_lm
# åŸºäºpytorchçš„ä¸­æ–‡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„DAPTå’ŒTAPT
[![License](https://img.shields.io/badge/license-Apache%202-green.svg)](https://www.apache.org/licenses/LICENSE-2.0)
[![Build Status](https://travis-ci.org/xialonghua/kotmvp.svg?branch=master)](https://travis-ci.org/xialonghua/kotmvp) 

ACL2020 Best Paperæœ‰ä¸€ç¯‡è®ºæ–‡æåå¥–ï¼Œã€ŠDonâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasksã€‹ã€‚è¿™ç¯‡è®ºæ–‡åšäº†å¾ˆå¤šè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„å®éªŒï¼Œç³»ç»Ÿçš„åˆ†æäº†è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå¯¹å­ä»»åŠ¡çš„æ•ˆæœæå‡æƒ…å†µã€‚æœ‰å‡ ä¸ªä¸»è¦ç»“è®ºï¼š
* åœ¨ç›®æ ‡é¢†åŸŸçš„æ•°æ®é›†ä¸Šç»§ç»­é¢„è®­ç»ƒï¼ˆDAPTï¼‰å¯ä»¥æå‡æ•ˆæœï¼›ç›®æ ‡é¢†åŸŸçš„è¯­æ–™ä¸RoBERTaçš„åŸå§‹é¢„è®­ç»ƒè¯­æ–™è¶Šä¸ç›¸å…³ï¼ŒDAPTæ•ˆæœåˆ™æå‡æ›´æ˜æ˜¾ã€‚

* åœ¨å…·ä½“ä»»åŠ¡çš„æ•°æ®é›†ä¸Šç»§ç»­é¢„è®­ç»ƒï¼ˆTAPTï¼‰å¯ä»¥ååˆ†â€œå»‰ä»·â€åœ°æå‡æ•ˆæœã€‚

* ç»“åˆäºŒè€…ï¼ˆå…ˆè¿›è¡ŒDAPTï¼Œå†è¿›è¡ŒTAPTï¼‰å¯ä»¥è¿›ä¸€æ­¥æå‡æ•ˆæœã€‚

* å¦‚æœèƒ½è·å–æ›´å¤šçš„ã€ä»»åŠ¡ç›¸å…³çš„æ— æ ‡æ³¨æ•°æ®ç»§ç»­é¢„è®­ç»ƒï¼ˆCurated-TAPTï¼‰ï¼Œæ•ˆæœåˆ™æœ€ä½³ã€‚

* å¦‚æœæ— æ³•è·å–æ›´å¤šçš„ã€ä»»åŠ¡ç›¸å…³çš„æ— æ ‡æ³¨æ•°æ®ï¼Œé‡‡å–ä¸€ç§ååˆ†è½»é‡åŒ–çš„ç®€å•æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œæ•ˆæœä¹Ÿä¼šæå‡ã€‚

**â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”ğŸ¤—å‰æ–¹é«˜èƒ½ğŸ¤—â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”**

å¯¹GPTã€GPT-2ã€GPTæ–‡æœ¬æ•°æ®é›†ä¸Šçš„è¯­è¨€å»ºæ¨¡åº“æ¨¡å‹è¿›è¡Œå¾®è°ƒ(æˆ–ä»å¤´å¼€å§‹è®­ç»ƒ)ã€‚ALBERT, BERT, distillbert, RoBERTa, XLNetâ€¦ GPTå’ŒGPT-2ä½¿ç”¨å› æœè¯­è¨€å»ºæ¨¡è¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒ  
(CLM)ä¸¢å¤±ï¼Œè€ŒALBERTã€BERTã€DistilBERTå’ŒRoBERTaä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡(MLM)è¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒçš„æŸå¤±ã€‚ XLNetä½¿ç”¨æ’åˆ—è¯­è¨€å»ºæ¨¡(PLM)ï¼Œå…³äºå®ƒä»¬ä¹‹é—´å·®å¼‚çš„æ›´å¤šä¿¡æ¯[æ¨¡å‹æ€»ç»“](https://huggingface.co/transformers/model_summary.html)

è¿™é‡Œæä¾›äº†ä¸¤ç»„è„šæœ¬ã€‚ ç¬¬ä¸€ä¸ªåˆ©ç”¨äº†Trainer APIã€‚ ç¬¬äºŒä¸ªå¸¦' no_trainer 'åç¼€çš„é›†åˆä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯å¹¶åˆ©ç”¨ğŸ¤—Accelerateåº“ã€‚ è¿™ä¸¤ä¸ªé›†åˆéƒ½ä½¿ç”¨ğŸ¤—Datasetsåº“ã€‚ å¦‚æœéœ€è¦å¯¹æ•°æ®é›†è¿›è¡Œé¢å¤–å¤„ç†ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è½»æ¾å®šåˆ¶å®ƒä»¬ã€‚
**PS:** æ—§ç‰ˆæœ¬çš„è„šæœ¬ `run_language_modeling.py` å¯ä»¥ä» [è¿™é‡Œ](https://github.com/huggingface/transformers/blob/main/examples/legacy/run_language_modeling.py). è·å–ã€‚

### GPT-2/GPT and causal language modeling
pre-training: causal language modeling

```bash
python run_clm.py \
    --model_name_or_path gpt2 \
    --train_file path_to_train_file \
    --validation_file path_to_validation_file \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm
```

è¿™ä½¿ç”¨å†…ç½®çš„HuggingFaceâ€œè®­ç»ƒå™¨â€è¿›è¡Œè®­ç»ƒã€‚ å¦‚æœæ‚¨æƒ³ä½¿ç”¨è‡ªå®šä¹‰çš„è®­ç»ƒå›è·¯ï¼Œæ‚¨å¯ä»¥åˆ©ç”¨æˆ–ä¿®æ”¹' run_clm_no_trainer.py 'è„šæœ¬ã€‚ æŸ¥çœ‹è„šæœ¬ä»¥è·å¾—å—æ”¯æŒçš„å‚æ•°åˆ—è¡¨ã€‚ ç¤ºä¾‹å¦‚ä¸‹: 

```bash
python run_clm_no_trainer.py \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --model_name_or_path gpt2 \
    --output_dir /tmp/test-clm
```

### RoBERTa/BERT/DistilBERT and masked language modeling

pre-training: masked language modeling.

æ ¹æ®RoBERTaçš„è®ºæ–‡ï¼Œä½¿ç”¨åŠ¨æ€å±è”½è€Œä¸æ˜¯é™æ€å±è”½ã€‚ å› æ­¤ï¼Œæ¨¡å‹å¯èƒ½æ”¶æ•›ç¨å¾®æ…¢ä¸€äº›(è¿‡åº¦æ‹Ÿåˆéœ€è¦æ›´å¤šçš„epochs)


```bash
python run_mlm.py \
    --model_name_or_path roberta-base \
    --train_file path_to_train_file \
    --validation_file path_to_validation_file \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-mlm
```

å¦‚æœæ‚¨çš„æ•°æ®é›†æ˜¯æ¯è¡Œä¸€ä¸ªæ ·æœ¬ç»„ç»‡çš„, éœ€è¦ä½¿ç”¨ `--line_by_line` 

è¿™ä½¿ç”¨å†…ç½®çš„HuggingFaceâ€œè®­ç»ƒå™¨â€è¿›è¡Œè®­ç»ƒã€‚ å¦‚æœæ‚¨æƒ³ä½¿ç”¨è‡ªå®šä¹‰çš„è®­ç»ƒå›è·¯ï¼Œæ‚¨å¯ä»¥åˆ©ç”¨æˆ–ä¿®æ”¹' run_mlm_no_trainer.py 'è„šæœ¬ã€‚ æŸ¥çœ‹è„šæœ¬ä»¥è·å¾—å—æ”¯æŒçš„å‚æ•°åˆ—è¡¨ã€‚ ç¤ºä¾‹å¦‚ä¸‹: 

```bash
python run_mlm_no_trainer.py \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --model_name_or_path roberta-base \
    --output_dir /tmp/test-mlm
```

**PS:** åœ¨TPUä¸Šï¼Œä½ åº”è¯¥ä½¿ç”¨æ ‡å¿— `--pad_to_max_length` å’Œ `--line_by_line` ç¡®ä¿æ‰€æœ‰çš„æ‰¹æ¬¡éƒ½æœ‰ç›¸åŒçš„é•¿åº¦.
 

### Whole word masking

é¦–å…ˆä½¿ç”¨LTPåˆ†è¯

```bash
export TRAIN_FILE=/path/to/train/file
export LTP_RESOURCE=/path/to/ltp/tokenizer
export BERT_RESOURCE=/path/to/bert/tokenizer
export SAVE_PATH=/path/to/data/ref.txt

python run_chinese_ref.py \
    --file_name=$TRAIN_FILE \
    --ltp=$LTP_RESOURCE \
    --bert=$BERT_RESOURCE \
    --save_path=$SAVE_PATH
```
ç„¶åæ‰§è¡Œè®­ç»ƒè„šæœ¬

```bash
export TRAIN_FILE=/path/to/train/file
export VALIDATION_FILE=/path/to/validation/file
export TRAIN_REF_FILE=/path/to/train/chinese_ref/file
export VALIDATION_REF_FILE=/path/to/validation/chinese_ref/file
export OUTPUT_DIR=/tmp/test-mlm-wwm

python run_mlm_wwm.py \
    --model_name_or_path roberta-base \
    --train_file $TRAIN_FILE \
    --validation_file $VALIDATION_FILE \
    --train_ref_file $TRAIN_REF_FILE \
    --validation_ref_file $VALIDATION_REF_FILE \
    --do_train \
    --do_eval \
    --output_dir $OUTPUT_DIR
```

### XLNet and permutation language modeling

XLNetä½¿ç”¨ä¸åŒçš„è®­ç»ƒç›®æ ‡ï¼Œå³æ’åˆ—è¯­è¨€å»ºæ¨¡ã€‚ è¿™æ˜¯ä¸€ç§è‡ªå›å½’æ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–è¾“å…¥çš„æ‰€æœ‰æ’åˆ—çš„æœŸæœ›å¯èƒ½æ€§æ¥å­¦ä¹ åŒå‘ä¸Šä¸‹æ–‡åºåˆ—åˆ†è§£é¡ºåºã€‚

ä½¿ç”¨ `--plm_probability` ç”¨äºæ’åˆ—è¯­è¨€å»ºæ¨¡çš„æ©ç tokençš„é•¿åº¦ä¸å‘¨å›´ä¸Šä¸‹æ–‡é•¿åº¦çš„æ¯”å€¼ã€‚

ä½¿ç”¨ `--max_span_length` é™åˆ¶ç”¨äºæ’åˆ—è¯­è¨€å»ºæ¨¡çš„å±è”½ä»¤ç‰Œçš„é•¿åº¦

```bash
python run_plm.py \
    --model_name_or_path=xlnet-base-cased \
    --train_file path_to_train_file \
    --validation_file path_to_validation_file \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-plm
```

å¦‚æœæ‚¨çš„æ•°æ®é›†æ˜¯æ¯è¡Œä¸€ä¸ªæ ·æœ¬ç»„ç»‡çš„, éœ€è¦ä½¿ç”¨ `--line_by_line` 

**PS:** åœ¨TPUä¸Šï¼Œä½ åº”è¯¥ä½¿ç”¨æ ‡å¿— `--pad_to_max_length` å’Œ `--line_by_line` ç¡®ä¿æ‰€æœ‰çš„æ‰¹æ¬¡éƒ½æœ‰ç›¸åŒçš„é•¿åº¦.


## Creating a model on the fly

ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹æ—¶ï¼Œé…ç½®å€¼å¯ä»¥é‡å†™  `--config_overrides`:


```bash
python run_clm.py --model_type gpt2 --tokenizer_name gpt2 \ --config_overrides="n_embd=1024,n_head=16,n_layer=48,n_positions=102" \
[...]
```

æ­¤åŠŸèƒ½ä»…åœ¨ `run_clm.py`, `run_plm.py` å’Œ `run_mlm.py`.




